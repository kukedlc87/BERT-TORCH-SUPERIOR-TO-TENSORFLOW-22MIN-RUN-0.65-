{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain_data = pd.read_csv('/kaggle/input/movie-genre-prediction/train.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-23T07:26:28.623904Z","iopub.execute_input":"2023-08-23T07:26:28.624273Z","iopub.status.idle":"2023-08-23T07:26:28.795745Z","shell.execute_reply.started":"2023-08-23T07:26:28.624241Z","shell.execute_reply":"2023-08-23T07:26:28.794577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #E8EAF6; padding: 20px; border-radius: 10px; box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1);\">\n    <h2 style=\"font-family: 'Verdana'; color: #3A405A;\">üîç PyTorch & Bert-Base-Uncased</h2>\n    <p style=\"font-size: 18px; font-family: 'Verdana'; color: #3A405A; line-height: 1.5em;\">This is my first approach to pretrained models.Before testing pretrained models, use tensorflow and classifiers such as catboost with accuracy of 0.30</p>\n</div>","metadata":{}},{"cell_type":"code","source":"\ntrain_data = train_data.drop(columns=[\"id\"])\n\nlabel_encoder = LabelEncoder()\n\n# Encoding\ny_train_encoded = label_encoder.fit_transform(train_data['genre'])\n\n# Model selection\nmodel_name = \"bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)\nembedding_model = BertModel.from_pretrained(model_name)\n\n# GPU for large task...\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nembedding_model.to(device)\n\n# Tokenizer & embeddings\nmax_length = 12\nconcatenated_text = train_data['synopsis'] + \" \" + train_data['movie_name']\nencoded_inputs = tokenizer(list(concatenated_text), padding='max_length', truncation=True, max_length=max_length, return_attention_mask=True)\n\n# Create DataLoader for train ds \ntrain_dataset = TensorDataset(torch.tensor(encoded_inputs['input_ids']), torch.tensor(encoded_inputs['attention_mask']), torch.tensor(y_train_encoded))\ntrain_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n\n# NETWORK definition,,\nclass CustomClassifier(nn.Module):\n    def __init__(self, embedding_model, num_classes):\n        super(CustomClassifier, self).__init__()\n        self.embedding_model = embedding_model\n        self.fc = nn.Linear(embedding_model.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        embeddings = self.embedding_model(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n        logits = self.fc(embeddings)\n        return logits\n\n# Create the new model aand move to GPU (device)\nnum_classes = len(label_encoder.classes_)\nmodel = CustomClassifier(embedding_model, num_classes)\nmodel.to(device)\n\n# Optimizer & loss\noptimizer = optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = nn.CrossEntropyLoss()\n\n# Train!!!!!!!\nmodel.train()\nfor epoch in range(4):  # Cambia el n√∫mero de √©pocas seg√∫n sea necesario\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/20\", leave=False)\n    total_correct = 0\n    total_samples = 0\n    \n    for batch in progress_bar:\n        optimizer.zero_grad()\n        input_ids, attention_mask, labels = [item.to(device) for item in batch]\n        logits = model(input_ids, attention_mask)\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n        \n        #  accuracy \n        _, predicted = torch.max(logits, 1)\n        total_correct += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n        accuracy = total_correct / total_samples\n        \n        progress_bar.set_postfix({\"loss\": loss.item(), \"accuracy\": accuracy})\n\n    # Accuracy * epoche\n    print(f'Epoch {epoch + 1} - Accuracy: {accuracy:.4f}')\n\n\n\n# Eval_model \nmodel.eval()\ntotal_correct = 0\ntotal_samples = 0\nwith torch.no_grad():\n    progress_bar = tqdm(train_loader, desc=\"Evaluating\", leave=False)\n    for batch in progress_bar:\n        input_ids, attention_mask, labels = [item.to(device) for item in batch]\n        logits = model(input_ids, attention_mask)\n        _, predicted = torch.max(logits, 1)\n        total_correct += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n        progress_bar.set_postfix({\"accuracy\": total_correct / total_samples})\n\naccuracy = total_correct / total_samples\nprint(f'Final Accuracy: {accuracy:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T07:26:28.812352Z","iopub.execute_input":"2023-08-23T07:26:28.812901Z","iopub.status.idle":"2023-08-23T07:55:25.676231Z","shell.execute_reply.started":"2023-08-23T07:26:28.812862Z","shell.execute_reply":"2023-08-23T07:55:25.674899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center>\n    <img src=\"https://img.freepik.com/free-photo/cute-ai-generated-cartoon-bunny_23-2150288886.jpg\" style=\"width: 250px; height: 250px; border-radius: 10px; box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.1);\">\n</center>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #FFF3E0; padding: 20px; border-radius: 10px; box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1);\">\n    <h3 style=\"font-family: 'Verdana'; color: #FF5733;\">OPEN TO COMMENTS & EDITS</h3>\n    ","metadata":{}}]}